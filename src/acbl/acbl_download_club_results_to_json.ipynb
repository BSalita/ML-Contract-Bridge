{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T13:48:37.155866Z",
     "start_time": "2021-09-02T13:48:37.143866Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-25T07:46:46.328304Z",
     "iopub.status.busy": "2021-04-25T07:46:46.328304Z",
     "iopub.status.idle": "2021-04-25T07:46:46.344277Z",
     "shell.execute_reply": "2021-04-25T07:46:46.343298Z",
     "shell.execute_reply.started": "2021-04-25T07:46:46.328304Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 6h\n",
    "# ACBL is throttling 45 club files at a time(?). Afterwards returns 403 forbidden. Their throttling is a moving target.\n",
    "# Performs following steps:\n",
    "# 1) Read club-results file for each club in list. List of clubs is predefined in config.py.\n",
    "# 2) Create list of club-result-detail files for each club.\n",
    "# 3) Download html file containing game details provided neither html nor json file exists.\n",
    "# 4) Save downloaded game details to local html file. They can be deleted/archived once json is created.\n",
    "# 5) Seach game details file for script tag containing embedded json info (var data = ...).\n",
    "# 6) Write json data to json file.\n",
    "\n",
    "# Next steps:\n",
    "# acbl-club-results-json-to-sql.ipynb creates a sql script file from each json file. The scripts are executed producing a single sql db.\n",
    "\n",
    "# Previous steps:\n",
    "# Requires a list of clubs numbers to be processed. See config.py for instructions.\n",
    "\n",
    "# todo:\n",
    "# implement error checking for request? Throttling produces errors such as 'Forbidden'.\n",
    "#    try\n",
    "#        r = requests.get(url)\n",
    "#        r.raise_for_status()\n",
    "#    except requests.exceptions.HTTPError as e:\n",
    "#        print (e.response.text)\n",
    "# automatically download newer club file if new month has started. download new data upto 1st day of current month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T13:48:39.585860Z",
     "start_time": "2021-09-02T13:48:37.157863Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-25T07:46:46.345274Z",
     "iopub.status.busy": "2021-04-25T07:46:46.345274Z",
     "iopub.status.idle": "2021-04-25T07:46:48.865317Z",
     "shell.execute_reply": "2021-04-25T07:46:48.864319Z",
     "shell.execute_reply.started": "2021-04-25T07:46:46.345274Z"
    }
   },
   "outputs": [],
   "source": [
    "import config # contains configurations/settings.\n",
    "import pandas as pd\n",
    "import pathlib\n",
    "import requests\n",
    "import re\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "from sqlalchemy import create_engine, inspect\n",
    "from sqlalchemy_utils.functions import database_exists, create_database\n",
    "import time\n",
    "import mlBridgeLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T13:48:39.600860Z",
     "start_time": "2021-09-02T13:48:39.586863Z"
    }
   },
   "outputs": [],
   "source": [
    "# override pandas display options\n",
    "mlBridgeLib.pd_options_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T13:48:39.616860Z",
     "start_time": "2021-09-02T13:48:39.602866Z"
    }
   },
   "outputs": [],
   "source": [
    "rootPath = pathlib.Path('e:/bridge/data')\n",
    "acblPath = rootPath.joinpath('acbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T13:50:21.477549Z",
     "start_time": "2021-09-02T13:48:39.618868Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-25T07:46:48.866282Z",
     "iopub.status.busy": "2021-04-25T07:46:48.866282Z",
     "iopub.status.idle": "2021-04-25T07:47:05.738354Z",
     "shell.execute_reply": "2021-04-25T07:47:05.737488Z",
     "shell.execute_reply.started": "2021-04-25T07:46:48.866282Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 2m to read 2517 existing (local file) clubs. 2h to download all files.\n",
    "# May require 3 or more attempts to get all urls.\n",
    "# DEL/S club-results\\*.html files to refresh club-results. e.g. Do DEL/S each month.\n",
    "# request club html files. Use read_local to either force downloading of html or reading local html.\n",
    "# There doesn't seem to be any permanent failures to read any club. Just try again.\n",
    "\n",
    "read_local = config.option_read_local\n",
    "#read_local = False # force requesting of all club html files from web. do this each month to update club results.\n",
    "htmls = {}\n",
    "total_clubs = len(config.option_club_numbers)\n",
    "failed_urls = []\n",
    "headers={\"user-agent\":None} # Not sure why this has become necessary\n",
    "for ncn,cn in enumerate(sorted(config.option_club_numbers)):\n",
    "    ncn += 1\n",
    "    url = config.option_base_url+str(cn)+'/'\n",
    "    file = url.replace(config.option_acbl_url,'')+str(cn)+'.html'\n",
    "    print(f'Processing file ({ncn}/{total_clubs}): {file}')\n",
    "    path = acblPath.joinpath(file)\n",
    "    if read_local and path.exists() and path.stat().st_size > 200:\n",
    "        html = path.read_text(encoding=\"utf-8\")\n",
    "        print(f'Reading local {file}: len={len(html)}')\n",
    "    else:\n",
    "        print(f'Requesting {url}')\n",
    "        r = requests.get(url,headers=headers)\n",
    "        html = r.text\n",
    "        print(f'Creating {file}: len={len(html)}')\n",
    "        if r.status_code != 200:\n",
    "            print(f'Error: status:{r.status_code} {url}')\n",
    "            time.sleep(60) # obsolete?\n",
    "            failed_urls.append(url)\n",
    "            continue\n",
    "        # pathlib.Path.mkdir(path.parent, parents=True, exist_ok=True)\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        path.write_text(html, encoding=\"utf-8\")\n",
    "        time.sleep(1) # need to self-throttle otherwise acbl returns 403 \"forbidden\". obsolete?\n",
    "    htmls[str(cn)] = html\n",
    "print(len(failed_urls),failed_urls)\n",
    "print(f\"Done: Total clubs processed:{total_clubs}: Total url failures:{len(failed_urls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T13:56:30.249564Z",
     "start_time": "2021-09-02T13:50:21.478552Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-25T07:47:05.740356Z",
     "iopub.status.busy": "2021-04-25T07:47:05.739358Z",
     "iopub.status.idle": "2021-04-25T07:48:38.095362Z",
     "shell.execute_reply": "2021-04-25T07:48:38.094386Z",
     "shell.execute_reply.started": "2021-04-25T07:47:05.740356Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 6m\n",
    "# Extract list of result html links embedded within each club's results file. Download any new results.\n",
    "\n",
    "dfs = {}\n",
    "ClubInfos = {}\n",
    "total_htmls = len(htmls)\n",
    "for n,(cn,html) in enumerate(htmls.items()):\n",
    "    n += 1\n",
    "    print(f'Processing club ({n}/{total_htmls}) {cn}')\n",
    "    bs = BeautifulSoup(html)\n",
    "    html_table = bs.find('table')\n",
    "    if html_table is None:\n",
    "        print(f'Invalid club-result for {cn}')\n",
    "        continue\n",
    "    # /html/body/div[2]/div/div[2]/div[1]/div[2]\n",
    "    ClubInfo = bs.find('div', 'col-md-8')\n",
    "    #print(ClubInfo)\n",
    "    ci = {}\n",
    "    ci['Name'] = ClubInfo.find('h1').contents[0].strip() # get first text and strip\n",
    "    ci['Location'] = ClubInfo.find('h5').contents[0].strip() # get first text and strip\n",
    "    if ClubInfo.find('a'):\n",
    "        ci['WebSite'] = ClubInfo.find('a')['href'] # get href of first a\n",
    "    ClubInfos[cn] = ci\n",
    "    print(f'{ci}')\n",
    "    # assumes first table is our target\n",
    "    d = pd.read_html(str(html_table))\n",
    "    assert len(d) == 1\n",
    "    df = pd.DataFrame(d[0])\n",
    "    df.insert(0,'Club',cn)\n",
    "    df.insert(1,'EventID','?')\n",
    "    hrefs = [config.option_acbl_url+link.get('href')[1:] for link in html_table.find_all('a', href=re.compile(\"^/club-results/details/\\d*$\"))]\n",
    "    df.drop('Unnamed: 6', axis=1, inplace=True)\n",
    "    df['ResultID'] = [result.rsplit('/', 1)[-1] for result in hrefs]\n",
    "    df['ResultUrl'] = hrefs\n",
    "    dfs[cn] = df\n",
    "print(f\"Done: Total clubs processed:{len(dfs)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-02T15:12:49.442082Z",
     "start_time": "2021-09-02T13:57:32.751801Z"
    },
    "execution": {
     "iopub.execute_input": "2021-04-25T07:48:38.096365Z",
     "iopub.status.busy": "2021-04-25T07:48:38.096365Z",
     "iopub.status.idle": "2021-04-25T19:33:55.063074Z",
     "shell.execute_reply": "2021-04-25T19:33:55.062108Z",
     "shell.execute_reply.started": "2021-04-25T07:48:38.096365Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 1h15m for zero updates. 6h for a 1 month update.\n",
    "# Skips files already downloaded. Ordering is newest result to oldest.\n",
    "# For each html result file, extract the embedded json data, write json to file.\n",
    "\n",
    "# todo:\n",
    "# sort enumerate(filtered_clubs.items())\n",
    "\n",
    "import time\n",
    "\n",
    "starting_nclub = 0 # beginning slice\n",
    "ending_nclub = 0 # ending slice. 0 means all files\n",
    "\n",
    "total_execution_time = 0 # todo:\n",
    "total_executed = 0 # todo:\n",
    "if ending_nclub == 0: ending_nclub = len(dfs)\n",
    "filtered_clubs = dfs # todo\n",
    "total_filtered_clubs = len(filtered_clubs)\n",
    "total_urls_processed = 0\n",
    "total_urls_to_process = '?' # todo: total sum of all result files in all clubs\n",
    "total_local_files = '?' # todo: total number of all local files to be processed\n",
    "total_local_files_read = 0\n",
    "start_time = time.time() # todo:\n",
    "\n",
    "total_clubs = len(filtered_clubs)\n",
    "failed_urls = []\n",
    "#headers={\"user-agent\":None} # Not sure why this has become necessary. Failed 2021-Sep-02 so using Chrome curl user-agent.\n",
    "headers={\"user-agent\":\"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.159 Safari/537.36\"}\n",
    "for ndf,(kdf,df) in enumerate(filtered_clubs.items()):\n",
    "    if ndf < starting_nclub or ndf >= ending_nclub:\n",
    "        print(f\"Skipping club #{ndf} {kdf}\") # obsolete when filtered_clubs works\n",
    "        continue\n",
    "    ndf += 1\n",
    "    total_results = len(df['ResultUrl'])\n",
    "    for cn, (nurl, url) in zip(df['Club'],enumerate(df['ResultUrl'])):\n",
    "        nurl += 1\n",
    "        total_urls_processed += 1\n",
    "        html_file = url.replace(config.option_acbl_url,'').replace('club-results','club-results/'+str(cn))+'.html'\n",
    "        json_file = html_file.replace('.html','.data.json')\n",
    "        print(f'Processing club ({ndf}/{total_clubs}): result file ({nurl}/{total_results}): {html_file}')\n",
    "        html_path = acblPath.joinpath(html_file)\n",
    "        json_path = acblPath.joinpath(json_file)\n",
    "        html = None\n",
    "        data_json = None\n",
    "        if config.option_read_local and json_path.exists():\n",
    "            if html_path.exists():\n",
    "                print(f'Found local html file: {html_file}')\n",
    "            else:\n",
    "                print(f'Missing local html file: {html_file}')\n",
    "            with open(json_path, 'r') as f:\n",
    "                data_json = json.load(f)\n",
    "            total_local_files_read += 1\n",
    "            print(f'Reading local ({total_local_files_read}/{total_local_files}) file:{json_path}: len:{json_path.stat().st_size}')\n",
    "        else:\n",
    "            print(f'Requesting {url}')\n",
    "            r = requests.get(url,headers=headers)\n",
    "            html = r.text\n",
    "            print(f'Creating {html_file}: len={len(html)}')\n",
    "            # some clubs return 200 (ok) but with instructions to login (len < 200).\n",
    "            # skip clubs returning errors or tiny files. assumes one failed club result will be true for all club's results.\n",
    "            if r.status_code != 200 or len(html) < 200:\n",
    "                failed_urls.append(url)\n",
    "                break\n",
    "            # pathlib.Path.mkdir(html_path.parent, parents=True, exist_ok=True)\n",
    "            html_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "            html_path.write_text(html, encoding=\"utf-8\")\n",
    "            bs = BeautifulSoup(html) # do this once and reuse? does it matter?\n",
    "            scripts = bs.find_all('script')\n",
    "            #print(scripts)\n",
    "            for script in scripts:\n",
    "                if script.string: # not defined for all scripts\n",
    "                    #print(script.string)\n",
    "                    vardata = re.search('var data = (.*);\\n', script.string)\n",
    "                    if vardata:\n",
    "                        data_json = json.loads(vardata.group(1))\n",
    "                        #print(json.dumps(data_json, indent=4))\n",
    "                        print(f\"Writing {json_path}\")\n",
    "                        with open(json_path, 'w') as f:\n",
    "                            json.dump(data_json, f, indent=2)\n",
    "                        bbo_tournament_id = data_json[\"bbo_tournament_id\"]\n",
    "                        print(f'bbo_tournament_id: {bbo_tournament_id}')\n",
    "            #time.sleep(1) # obsolete?\n",
    "        # if no data_json file read, must be an error so delete both html and json files.\n",
    "        if not data_json:\n",
    "            html_path.unlink(missing_ok=True)\n",
    "            json_path.unlink(missing_ok=True)\n",
    "        print(f'Files processed ({total_urls_processed}/{total_local_files_read}/{total_urls_to_process})')\n",
    "print(len(failed_urls),failed_urls)\n",
    "print(f\"Done: Totals: clubs:{total_clubs} urls:{total_urls_processed} local files read:{total_local_files_read}: failed urls:{len(failed_urls)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "output_auto_scroll": true
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d437d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T09:16:09.894523Z",
     "start_time": "2021-07-27T09:16:09.891502Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 30m to process 500,000 player-session files.\n",
    "# Performs following steps:\n",
    "# 1) Load player/*.session.json files.\n",
    "# 2) Creating sql-friendly dictionaries from json files.\n",
    "# 3) Iterate over json dictionaries producing a sql file for each json file.\n",
    "# 4) Read all the sql files to create an in-memory database.\n",
    "#    Creating an in-memory db, then writing to disk is 100x faster then directly creating a db on disk.\n",
    "# 5) Write the in-memory database to a single disk file (>4GB).\n",
    "\n",
    "# next steps:\n",
    "# acbl-sql-to-tournament-sessions.ipynb (not written yet) reads SQL tables\n",
    "\n",
    "# previous steps:\n",
    "# acbl_download_tournaments.ipynb\n",
    "\n",
    "# todo:\n",
    "# Clean up sql tables. Change NULL to NOT NULL. Add/remove PRIMARY KEY? Change VARCHAR to INT, REAL, etc.\n",
    "# Improve autogeneration of sql table file.\n",
    "# Improve how json_to_sql_walk() and CreateSqlFile() handles ids. Effect ON CONFLICT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d5f18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T09:16:14.233013Z",
     "start_time": "2021-07-27T09:16:11.254829Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import config # contains configurations/settings.import pandas as pd\n",
    "import pathlib\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import mlBridgeLib\n",
    "\n",
    "import sqlalchemy\n",
    "import sqlalchemy_utils\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f501fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T09:16:14.249016Z",
     "start_time": "2021-07-27T09:16:14.235016Z"
    }
   },
   "outputs": [],
   "source": [
    "# override pandas display options\n",
    "mlBridgeLib.pd_options_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718636b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T09:16:14.764389Z",
     "start_time": "2021-07-27T09:16:14.754757Z"
    }
   },
   "outputs": [],
   "source": [
    "rootPath = pathlib.Path('e:/bridge/data')\n",
    "acblPath = rootPath.joinpath('acbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac74f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T09:16:15.916099Z",
     "start_time": "2021-07-27T09:16:15.898096Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple function to walk a json file printing keys and values.\n",
    "# usage: json_walk('main',data_json) where 'main' is first table and data_json is a string containing json.\n",
    "\n",
    "def json_walk_print(key,value):\n",
    "    if type(value) is dict:\n",
    "        #print('dict:'+key)\n",
    "        for k,v in value.items():\n",
    "            kk = key+'.'+k\n",
    "            json_walk_print(kk,v)\n",
    "    elif type(value) is list:\n",
    "        #print('list:'+key)\n",
    "        for n,v in enumerate(value):\n",
    "            kk = key+'['+str(n)+']'\n",
    "            json_walk_print(kk,v)\n",
    "    else:\n",
    "        if type(value) is str:\n",
    "            value = '\"'+value+'\"'\n",
    "        print(key+'='+str(value))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada544f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T13:31:08.045806Z",
     "start_time": "2021-07-27T13:31:08.013140Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# walk a json file building a table suitable for generating SQL statements.\n",
    "# usage: json_to_sql_walk(tables,'main',data_json,primary_keys) where 'main' is first table and data_json is a string containing json.\n",
    "\n",
    "def sql_create_tables(tables,key,value):\n",
    "    #print(tables, key, value)\n",
    "    #print(f\"{key}={value}\")\n",
    "    splited = key.split('.')\n",
    "    tableName = splited[-3]\n",
    "    fieldId = splited[-2]\n",
    "    fieldName = splited[-1]\n",
    "    #print(\"ct:\", tableName, fieldId, fieldName, type(value))\n",
    "    # removed assert as they were json schema specific\n",
    "    #assert not tableName[0].isdigit(), [tableName, fieldId, fieldName, type(value)]\n",
    "    #assert fieldId[0].isdigit(), [tableName, fieldId, fieldName, type(value)]\n",
    "    #assert not fieldName[0].isdigit(), [tableName, fieldId, fieldName, type(value)]\n",
    "    if fieldName in tables[tableName][fieldId]:\n",
    "        #print(type(tables[tableName][fieldId][fieldName]))\n",
    "        #print(tableName,fieldId,fieldName)\n",
    "        assert type(tables[tableName][fieldId][fieldName]) is list\n",
    "        if type(value) is list:\n",
    "            tables[tableName][fieldId][fieldName] += value\n",
    "        else:\n",
    "            tables[tableName][fieldId][fieldName].append(value)\n",
    "        # set will return unique values from list but all must be same type e.g. str\n",
    "        tables[tableName][fieldId][fieldName] = list(set(tables[tableName][fieldId][fieldName])) # force unique. award pigment issue\n",
    "    else:\n",
    "        tables[tableName][fieldId][fieldName] = value\n",
    "    return\n",
    "\n",
    "def json_to_sql_walk(tables,key,last_id,uid,value,primary_keys):\n",
    "    #print(tables,key,last_id,uid,value)\n",
    "    if type(value) is dict:\n",
    "        #print('dict:',key,uid)\n",
    "        if any([pk in value for pk in primary_keys]):\n",
    "            for pk in primary_keys:\n",
    "                if pk in value:\n",
    "                    last_id = key.split('.')[-1]\n",
    "                    uid = [str(value[pk])]\n",
    "                    if key.count('.') > 0:\n",
    "                        sql_create_tables(tables,key,'-'.join(uid))\n",
    "        elif all(not k.isdigit() for k in value.keys()):\n",
    "            sql_create_tables(tables,key+'.'+'-'.join(uid)+'.id','-'.join(uid)) # create PRIMARY KEY column of 'id'\n",
    "            sql_create_tables(tables,key+'.'+'-'.join(uid)+'.'+last_id,uid[0]) # create parent column using last_id and uid[0] (first id)\n",
    "            if key.count('.') > 0:\n",
    "                sql_create_tables(tables,key,['-'.join(uid)])\n",
    "        for k,v in value.items():\n",
    "            if all(kk.isdigit() for kk in value.keys()):\n",
    "                json_to_sql_walk(tables,key,last_id,uid+[k],v,primary_keys)\n",
    "            else:\n",
    "                json_to_sql_walk(tables,key+'.'+'-'.join(uid)+'.'+k,last_id,uid,v,primary_keys)\n",
    "    elif type(value) is list:\n",
    "        #print('list:',key,uid)\n",
    "        if len(value) > 0: # turn empty lists into NULL?\n",
    "            #print(\"empty list:\",key)\n",
    "            sql_create_tables(tables,key,[])\n",
    "        for n,v in enumerate(value):\n",
    "            json_to_sql_walk(tables,key,last_id,uid+[str(n)],v,primary_keys)\n",
    "    else:\n",
    "        sql_create_tables(tables,key,value)\n",
    "    return\n",
    "\n",
    "#tables = defaultdict(lambda :defaultdict(dict))\n",
    "#json_to_sql_walk(tables,'events',\"\",[],data_json,primary_keys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa506b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T13:55:10.015291Z",
     "start_time": "2021-07-27T13:55:10.003905Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a file of SQL INSERT commands from table\n",
    "\n",
    "def CreateSqlFile(tables,f,primary_keys):\n",
    "    print(\"PRAGMA foreign_keys = OFF;\", file=f) # is this still necessary????\n",
    "    \n",
    "    for k,v in tables.items():\n",
    "        assert type(v) is defaultdict\n",
    "        #print(f\"DELETE FROM [{k}];\", file=f) # delete all rows\n",
    "        for kk,vv in v.items():\n",
    "            assert type(vv) is dict\n",
    "            s = '\\\",\\\"'.join(vvv for vvv in vv.keys()) # backslashes can't be included within format {}\n",
    "            print(f\"INSERT INTO \\\"{k}\\\" (\\\"{s}\\\")\", file=f)\n",
    "            values = []\n",
    "            for kkk,vvv in vv.items():\n",
    "                #print(kkk,vvv)\n",
    "                if type(vvv) is str:\n",
    "                    values.append('\\''+vvv.replace('\\'','\\'\\'')+'\\'') # escape embedded double-quotes with sql's double double-quotes\n",
    "                elif vvv is None:\n",
    "                    values.append(\"NULL\")\n",
    "                elif type(vvv) is list:\n",
    "                    #print(\"list:\",kkk)\n",
    "                    values.append('\\'['+','.join(str(vvvv).replace('\\'','\\'\\'') for vvvv in vvv)+']\\'') # escape embedded double-quotes with sql's double double-quotes\n",
    "                else:\n",
    "                    values.append(vvv)\n",
    "            print(f\"VALUES({','.join(str(vvvv) for vvvv in values)})\", file=f)\n",
    "            # DO UPDATE SET updated_at=excluded.updated_at\n",
    "            s = ','.join('\\\"'+vvv+'\\\"=excluded.\\\"'+vvv+'\\\"' for vvv in vv.keys()) # backslashes can't be included within format {}\n",
    "            assert any([pk in vv for pk in primary_keys]),[primary_keys,vv]\n",
    "            for pk in primary_keys:\n",
    "                if pk in vv:\n",
    "                    print(f\"ON CONFLICT({pk}) DO UPDATE SET {s}\", file=f, end='')\n",
    "            #print('created_at' in vv.keys(),'updated_at' in vv.keys())\n",
    "            assert ('created_at' in vv.keys()) == ('updated_at' in vv.keys())\n",
    "            if 'created_at' in vv.keys():\n",
    "                print(f\"\\nWHERE excluded.\\\"updated_at\\\" > \\\"updated_at\\\" OR (excluded.\\\"updated_at\\\" = \\\"updated_at\\\" AND excluded.\\\"created_at\\\" > \\\"created_at\\\")\", file=f, end='')\n",
    "            print(\";\\n\",file=f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7358eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T15:21:02.676473Z",
     "start_time": "2021-07-27T14:56:57.514770Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# takes 7m to 140m to process 320,000 files.\n",
    "# takes 2m, or 7m to update 1 new month of 12,000 files or 11000s if starting from scratch (3h?).\n",
    "# todo:\n",
    "# how to get more more exception info in try/except?\n",
    "# move acbl_tournament_sessions.sql into options\n",
    "# move partial directory and rglob() into options\n",
    "# create cookbook of sql queries.\n",
    "# write sql script that creates tables and columns similar to Excel implementations.\n",
    "# initially delete all urls or just filtered urls. if filtered, then need to pass to execute_sql() below.\n",
    "# group urls by directory so number of files in directory can be displayed.\n",
    "# show overall file progress, dir to process, files within each directory to process:(1234/20000) dir:(3/266) file:(123/300)\n",
    "# if skip_existing_files and file exists, do a rglob for .sql files, create list of .json files not in .sql list.\n",
    "#     ... will be faster than a for loop.\n",
    "\n",
    "initially_delete_all_output_files = False\n",
    "skip_existing_files = True\n",
    "starting_nfile = 0\n",
    "ending_nfile = 0\n",
    "event_types = [] # 'PAIRS', 'HOME_STYLE_PAIRS', 'INDIVIDUAL', 'TEAMS'\n",
    "#event_types = ['TEAMS']\n",
    "\n",
    "urls = []\n",
    "for path in acblPath.joinpath('players').rglob('*.session.json'): # fyi: PurePathPosix doesn't support glob/rglob\n",
    "    urls.append(path)\n",
    "\n",
    "total_execution_time = 0\n",
    "total_files_written = 0\n",
    "if ending_nfile == 0: ending_nfile = len(urls)\n",
    "filtered_urls = urls[starting_nfile:ending_nfile]\n",
    "total_urls = len(filtered_urls)\n",
    "start_time = time.time()\n",
    "\n",
    "# delete files first, using filtered list of urls\n",
    "if initially_delete_all_output_files:\n",
    "    for nfile,url in enumerate(filtered_urls):\n",
    "        sql_file = url.with_suffix('.sql')\n",
    "        sql_file.unlink(missing_ok=True)\n",
    "\n",
    "for nfile,url in enumerate(filtered_urls):\n",
    "    nfile += 1\n",
    "    json_file = url\n",
    "    sql_file = url.with_suffix('.sql')\n",
    "    print(f\"Processing ({nfile}/{total_urls}): file:{json_file.as_posix()}\")\n",
    "    if skip_existing_files:\n",
    "        if sql_file.exists():\n",
    "           print(f\"Skipping: File exists:{sql_file.as_posix()}\")\n",
    "           continue\n",
    "    #try:\n",
    "        data_json = None\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_json = json.load(f)\n",
    "            #engine = sqlalchemy.create_engine(db_connection_string, echo=create_engine_echo)\n",
    "            #raw_connection = engine.raw_connection()\n",
    "            #create_tables_sql_file = \"acbl_tournament_sessions.sql\"\n",
    "            #db_file = 'acbl_tournament_sessions.sqlite'\n",
    "            #db_file_connection_string = 'sqlite:///'+acblPath.joinpath(db_file).as_posix()\n",
    "            #engine = sqlalchemy.create_engine(db_file_connection_string, echo=False)\n",
    "            #db = pd.read_json(f)\n",
    "            #db.to_sql('sessions',con=engine)\n",
    "            #error\n",
    "        # hmmm, the obvious id, acbl_number, is not in the json file. However, it is the name of a parent directory so use it.\n",
    "        data_json['acbl_number'] = json_file.parent.parent.stem\n",
    "        data_json['acbl_number_session_id'] = data_json['acbl_number']+'_'+data_json['session_id']\n",
    "        #print(f\"Reading {json_file.as_posix()} dict len:{len(data_json)}\")\n",
    "        #if len(event_types) > 0 and data_json['type'] not in event_types:\n",
    "        #    #print(f\"Skipping type:{data_json['type']}: file{json_file.as_posix()}\")\n",
    "        #    continue\n",
    "        tables = defaultdict(lambda :defaultdict(dict))\n",
    "        primary_keys = ['acbl_number_session_id','id']\n",
    "        json_to_sql_walk(tables,\"sessions\",\"\",\"\",data_json,primary_keys) # \"sessions\" is the main table.\n",
    "        with open(sql_file,'w') as f:\n",
    "            CreateSqlFile(tables,f,primary_keys)\n",
    "        total_files_written += 1\n",
    "    #except Exception as e:\n",
    "        #print(f\"Error: {e}: session_id:{data_json['session_id']} file:{url.as_posix()}\")\n",
    "    #else:\n",
    "        print(f\"Writing: session_id:{data_json['session_id']} file:{sql_file.as_posix()}\")\n",
    "\n",
    "print(f\"All files processed:{total_urls} files written:{total_files_written} total time:{round(time.time()-start_time,2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ecd82f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T15:21:02.708476Z",
     "start_time": "2021-07-27T15:21:02.677480Z"
    }
   },
   "outputs": [],
   "source": [
    "# Automatically create sql tables file.\n",
    "# Will require further editing of most fields: fix PRIMARY KEY, make NOT NULL, change VARTYPE to INT, REAL, remove trailing comma.\n",
    "\n",
    "def CreateSqlTablesFile(f,tables,primary_keys):\n",
    "    assert type(tables) is defaultdict\n",
    "    print(f'PRAGMA journal_mode=WAL;', file=f)\n",
    "    print(file=f)\n",
    "    for k,v in tables.items():\n",
    "        print(f'DROP TABLE IF EXISTS \"{k}\";', file=f)\n",
    "        print(file=f)\n",
    "        print(f'CREATE TABLE \"{k}\" (', file=f)\n",
    "        assert type(v) is defaultdict\n",
    "        for kk,vv in v.items():\n",
    "            #print('uid:','.'.join([k,kk]))\n",
    "            assert kk[0].isdigit()\n",
    "            assert type(vv) is dict\n",
    "            for kkk,vvv in vv.items():\n",
    "                #print('3:','.'.join([k,kkk]))\n",
    "                assert type(vvv) is not list or type(vvv) is not dict\n",
    "                if kkk in primary_keys:\n",
    "                    print(f'\"{kkk}\" INT NOT NULL PRIMARY KEY,', file=f)\n",
    "                else:\n",
    "                    print(f'\"{kkk}\" VARCHAR NULL,', file=f) # or NOT\n",
    "        for kkk,vvv in vv.items():\n",
    "            #print(kkk,tables.keys())\n",
    "            if kkk in tables:\n",
    "                print(f'-- list of VARCHAR', file=f)\n",
    "                print(f'FOREIGN KEY (\"{kkk}\") REFERENCES \"{kkk}\"(id) ON DELETE NO ACTION,', file=f)\n",
    "        print(');', file=f)\n",
    "        print(file=f)\n",
    "\n",
    "# only create file if it doesn't exist. Must manually delete file if new version is wanted.\n",
    "create_tables_sql_file = pathlib.Path(\"acbl_tournament_sessions.sql\")\n",
    "if not create_tables_sql_file.exists():\n",
    "    with open(create_tables_sql_file,'w') as f:\n",
    "        CreateSqlTablesFile(f,tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad2a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-27T16:20:54.382106Z",
     "start_time": "2021-07-27T15:21:27.199501Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# notes:\n",
    "# takes 9h to process 300,000 files into 40GB sql file. Each sql file is processed in about .05s. Windows 11 2700x system.\n",
    "# 1) creating in memory db, executing scripts, then writing memory db to disk file is 100x faster than direct writes to file.\n",
    "# 2) Can be incredibly slow unless \"PRAGMA journal_mode=WAL;\" is used. Much, much faster with PRAGMA.\n",
    "# You can manually set, using sqlite3 command, journal_mode to WAL once and it will be persisted.\n",
    "# .open <file>\n",
    "# PRAGMA journal_mode=WAL;\n",
    "# .quit\n",
    "# Alternatively, connection can be opened in WAL mode. e.g. new SQLiteConnection(\"Data Source=\" + file + \";PRAGMA journal_mode=WAL;\")\n",
    "# None of these statements improve performance:\n",
    "#   \"PRAGMA locking_mode=exclusive;\"\n",
    "#   \"PRAGMA synchronous=NORMAL;\"\n",
    "#   db.commit()\n",
    "\n",
    "# todo:\n",
    "# Should .sql files use TRANSACTION and COMMIT to prevent partial updates?\n",
    "# collect errors, report at finish, sort by error type.\n",
    "# run VALIDATE or CHECK before/after file save?\n",
    "\n",
    "# options\n",
    "starting_nfile = 0 # beginning slice\n",
    "ending_nfile = 0 # ending slice. 0 means all files\n",
    "write_direct_to_disk = False # write directly to disk. Otherwise will write to memory and then backup to disk (100x faster).\n",
    "create_tables = True # drop and recreate all tables. Will wipe out all data.\n",
    "delete_db = True\n",
    "create_engine_echo = False\n",
    "perform_integrity_checks = False\n",
    "db_file = 'acbl_tournament_sessions.sqlite'\n",
    "db_memory_connection_string = 'sqlite://'\n",
    "create_tables_sql_file = \"acbl_tournament_sessions.sql\"\n",
    "\n",
    "db_file_connection_string = 'sqlite:///'+acblPath.joinpath(db_file).as_posix()\n",
    "db_file_path = acblPath.joinpath(db_file)\n",
    "\n",
    "if write_direct_to_disk:\n",
    "    db_connection_string = db_file_connection_string # disk file based db\n",
    "else:\n",
    "    db_connection_string = db_memory_connection_string # memory based db\n",
    "\n",
    "if delete_db and sqlalchemy_utils.functions.database_exists(db_file_connection_string):\n",
    "    print(f\"Deleting db:{db_file_connection_string}\")\n",
    "    sqlalchemy_utils.functions.drop_database(db_file_connection_string) # warning: can't delete file if in use by another app (restart kernel).\n",
    "\n",
    "if not sqlalchemy_utils.functions.database_exists(db_connection_string):\n",
    "    print(f\"Creating db:{db_connection_string}\")\n",
    "    sqlalchemy_utils.functions.create_database(db_connection_string)\n",
    "    create_tables = True\n",
    "    \n",
    "engine = sqlalchemy.create_engine(db_connection_string, echo=create_engine_echo)\n",
    "raw_connection = engine.raw_connection()\n",
    "\n",
    "if create_tables:\n",
    "    print(f\"Creating tables from:{create_tables_sql_file}\")\n",
    "    with open(create_tables_sql_file, 'r') as f:\n",
    "        create_sql = f.read()\n",
    "    raw_connection.executescript(create_sql) # create tables\n",
    "\n",
    "urls = []\n",
    "for path in acblPath.joinpath('players').rglob('*.session.sql'): # fyi: PurePathPosix doesn't support glob/rglob\n",
    "    urls.append(path)\n",
    "\n",
    "total_script_execution_time = 0\n",
    "total_scripts_executed = 0\n",
    "canceled = False\n",
    "if ending_nfile == 0: ending_nfile = len(urls)\n",
    "filtered_urls = urls[starting_nfile:ending_nfile]\n",
    "total_filtered_urls = len(filtered_urls)\n",
    "start_time = time.time()\n",
    "for nfile,url in enumerate(filtered_urls):\n",
    "    nfile += 1\n",
    "    sql_file = url\n",
    "    print(f\"Executing SQL script ({nfile}/{total_filtered_urls}): file:{sql_file.as_posix()}\")\n",
    "    \n",
    "    try:\n",
    "        sql_script = None\n",
    "        with open(sql_file, 'r') as f:\n",
    "            sql_script = f.read()\n",
    "        start_script_time = time.time()\n",
    "        raw_connection.executescript(sql_script)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {type(e).__name__} while processing file:{url.as_posix()}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Removing {url.as_posix()}\")\n",
    "        error\n",
    "        sql_file.unlink(missing_ok=True) # delete any bad files, fix issues, rerun.\n",
    "        continue # todo: log error.\n",
    "        #break\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(f\"Error: {type(e).__name__} while processing file:{url.as_posix()}\")\n",
    "        print(traceback.format_exc())\n",
    "        canceled = True\n",
    "        break\n",
    "    else:\n",
    "        script_execution_time = time.time()-start_script_time\n",
    "        print(f\"SQL script executed: file:{url.as_posix()}: time:{round(script_execution_time,2)}\")\n",
    "        total_script_execution_time += script_execution_time\n",
    "        total_scripts_executed += 1\n",
    "\n",
    "print(f\"SQL scripts executed ({total_scripts_executed}/{total_filtered_urls}/{len(urls)}): total changes:{raw_connection.total_changes} total script execution time:{round(time.time()-start_time,2)}: avg script execution time:{round(total_script_execution_time/max(1,total_scripts_executed),2)}\")\n",
    "# if using memory db, write memory db to disk file.\n",
    "if not canceled:\n",
    "    if perform_integrity_checks:\n",
    "        # todo: research how to detect and display failures? Which checks are needed?\n",
    "        print(f\"Performing quick_check on file\")\n",
    "        raw_connection.execute(\"PRAGMA quick_check;\") # takes 7m on disk\n",
    "        print(f\"Performing foreign_key_check on file\")\n",
    "        raw_connection.execute(\"PRAGMA foreign_key_check;\") # takes 3m on disk\n",
    "        print(f\"Performing integrity_check on file\")\n",
    "        raw_connection.execute(\"PRAGMA integrity_check;\") # takes 25m on disk\n",
    "    if not write_direct_to_disk:\n",
    "        print(f\"Writing memory db to file:{db_file_connection_string}\")\n",
    "        engine_file = sqlalchemy.create_engine(db_file_connection_string)\n",
    "        raw_connection_file = engine_file.raw_connection()\n",
    "        raw_connection.backup(raw_connection_file.connection)\n",
    "        raw_connection_file.close()\n",
    "        engine_file.dispose()\n",
    "\n",
    "raw_connection.close()\n",
    "engine.dispose()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfdde55",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "output_auto_scroll": true,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

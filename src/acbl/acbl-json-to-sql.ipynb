{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d437d3b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T14:30:57.341403Z",
     "start_time": "2021-07-22T14:30:57.335409Z"
    }
   },
   "outputs": [],
   "source": [
    "# takes 4h to process 300,000 files.\n",
    "# Performs following steps:\n",
    "# 1) Load club-results-details json files.\n",
    "# 2) Creating sql-friendly dictionaries from json files.\n",
    "# 3) Iterate over dictionaries producing a sql file for each json file.\n",
    "# 4) Read all the sql files to create an in-memory database.\n",
    "#    Creating an in-memory db, then writing to disk is 100x faster then directly creating a db on disk.\n",
    "# 5) Write the in-memory database to a single disk file (>25GB).\n",
    "\n",
    "# next steps:\n",
    "# acbl-sql-to-board-results.ipynb reads SQL tables, creates a new db with board/player data structured for analysis.\n",
    "\n",
    "# previous steps:\n",
    "# acbl-details-to-json.ipynb\n",
    "\n",
    "# todo:\n",
    "# Fix NULL constraint issues when creating sql file. Losing 20,000 results due to BBO NULL constraint issue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d79d5f18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-22T14:30:57.724470Z",
     "start_time": "2021-07-22T14:30:57.582419Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import config # contains configurations/settings.import pandas as pd\n",
    "import pathlib\n",
    "import time\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import requests\n",
    "import mlBridgeLib\n",
    "\n",
    "import sqlalchemy\n",
    "import sqlalchemy_utils\n",
    "\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f501fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T08:50:08.284260Z",
     "start_time": "2021-07-09T08:50:08.270261Z"
    }
   },
   "outputs": [],
   "source": [
    "# override pandas display options\n",
    "mlBridgeLib.pd_options_display()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "718636b7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T08:50:08.300261Z",
     "start_time": "2021-07-09T08:50:08.286261Z"
    }
   },
   "outputs": [],
   "source": [
    "rootPath = pathlib.Path('e:/bridge/data')\n",
    "acblPath = rootPath.joinpath('acbl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ac74f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T08:50:08.316258Z",
     "start_time": "2021-07-09T08:50:08.302260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# simple function to walk a json file printing keys and values.\n",
    "# usage: json_walk('main',data_json) where 'main' is first table and data_json is a string containing json.\n",
    "\n",
    "def json_walk_print(key,value):\n",
    "    if type(value) is dict:\n",
    "        #print('dict:'+key)\n",
    "        for k,v in value.items():\n",
    "            kk = key+'.'+k\n",
    "            json_walk_print(kk,v)\n",
    "    elif type(value) is list:\n",
    "        #print('list:'+key)\n",
    "        for n,v in enumerate(value):\n",
    "            kk = key+'['+str(n)+']'\n",
    "            json_walk_print(kk,v)\n",
    "    else:\n",
    "        if type(value) is str:\n",
    "            value = '\"'+value+'\"'\n",
    "        print(key+'='+str(value))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ada544f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T08:50:08.332262Z",
     "start_time": "2021-07-09T08:50:08.318261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# walk a json file building a table suitable for generating SQL statements.\n",
    "# usage: json_to_sql_walk(tables,'main',data_json) where 'main' is first table and data_json is a string containing json.\n",
    "\n",
    "# Tables containing dicts are declared with Foreign Key.\n",
    "# note: bboGameLinks, points and stratvalues have no id. had to be inserted. points is not in a list.\n",
    "# awards[pigment(list of dict(id))]\n",
    "# award_scores[pigment(list of dict(id)]\n",
    "# board_results[board_results_add_ons(id)][board_results_addons(id)]\n",
    "# boards[board_results(list of dict(id)),movieExists(list of dict(id))]\n",
    "# PAIR[bboGameLinks(dict of dicts(no id)][club(id)][game_type(id)][sessions(list of dicts(id))[strats(list of dicts(id))[stratvalues(dict of dicts(no id)][uploads(list of dicts(id))\n",
    "# hand_record[points(no list,no id)]\n",
    "# pair_summaries[players(list of dicts(id))][strat_place(list of dicts(id))][session_scores(list of dicts(id))]\n",
    "# players[award_scores(list of dicts(id))][awards(list of dicts(id))]\n",
    "# sections[pair_summaries(list of dicts(id))][boards(list of dicts(id))]\n",
    "# sessions[hand_records(list of dicts(id))][sections(list of dicts(id))]\n",
    "# if skip_existing_files and file exists, do a rglob for .json files, create list of .html files not in .json list.\n",
    "#     ... will be faster than a for loop.\n",
    "\n",
    "def sql_create_tables(tables,key,value):\n",
    "    #print(f\"{key}={value}\")\n",
    "    splited = key.split('.')\n",
    "    tableName = splited[-3]\n",
    "    fieldId = splited[-2]\n",
    "    fieldName = splited[-1]\n",
    "    #print(\"ct:\", tableName, fieldId, fieldName, type(value))\n",
    "    assert not tableName[0].isdigit(), [tableName, fieldId, fieldName, type(value)]\n",
    "    assert fieldId[0].isdigit(), [tableName, fieldId, fieldName, type(value)]\n",
    "    assert not fieldName[0].isdigit(), [tableName, fieldId, fieldName, type(value)]\n",
    "    if fieldName in tables[tableName][fieldId]:\n",
    "        #print(type(tables[tableName][fieldId][fieldName]))\n",
    "        assert type(tables[tableName][fieldId][fieldName]) is list\n",
    "        if type(value) is list:\n",
    "            tables[tableName][fieldId][fieldName] += value\n",
    "        else:\n",
    "            tables[tableName][fieldId][fieldName].append(value)\n",
    "        # set will return unique values from list but all must be same type e.g. str\n",
    "        tables[tableName][fieldId][fieldName] = list(set(tables[tableName][fieldId][fieldName])) # force unique. award pigment issue\n",
    "    else:\n",
    "        tables[tableName][fieldId][fieldName] = value\n",
    "    return\n",
    "\n",
    "def json_to_sql_walk(tables,key,last_id,uid,value):\n",
    "    if type(value) is dict:\n",
    "        #print('dict:',key,uid)\n",
    "        if 'id' in value:\n",
    "            last_id = key.split('.')[-1]\n",
    "            uid = [str(value['id'])]\n",
    "            if key.count('.') > 0:\n",
    "                sql_create_tables(tables,key,'-'.join(uid))\n",
    "        elif all(not k.isdigit() for k in value.keys()):\n",
    "            sql_create_tables(tables,key+'.'+'-'.join(uid)+'.id','-'.join(uid)) # create PRIMARY KEY column of 'id'\n",
    "            sql_create_tables(tables,key+'.'+'-'.join(uid)+'.'+last_id,uid[0]) # create parent column using last_id and uid[0] (first id)\n",
    "            if key.count('.') > 0:\n",
    "                sql_create_tables(tables,key,['-'.join(uid)])\n",
    "        for k,v in value.items():\n",
    "            if all(kk.isdigit() for kk in value.keys()):\n",
    "                json_to_sql_walk(tables,key,last_id,uid+[k],v)\n",
    "            else:\n",
    "                json_to_sql_walk(tables,key+'.'+'-'.join(uid)+'.'+k,last_id,uid,v)\n",
    "    elif type(value) is list:\n",
    "        #print('list:',key,uid)\n",
    "        if len(value) > 0: # turn empty lists into NULL?\n",
    "            #print(\"empty list:\",key)\n",
    "            sql_create_tables(tables,key,[])\n",
    "        for n,v in enumerate(value):\n",
    "            json_to_sql_walk(tables,key,last_id,uid+[str(n)],v)\n",
    "    else:\n",
    "        sql_create_tables(tables,key,value)\n",
    "    return\n",
    "\n",
    "#tables = defaultdict(lambda :defaultdict(dict))\n",
    "#json_to_sql_walk(tables,'events',\"\",[],data_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fa506b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T08:50:08.348258Z",
     "start_time": "2021-07-09T08:50:08.334263Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create a file of SQL INSERT commands from table\n",
    "\n",
    "def CreateSqlFile(tables,f):\n",
    "    print(\"PRAGMA foreign_keys = OFF;\", file=f) # is this still necessary????\n",
    "    \n",
    "    for k,v in tables.items():\n",
    "        assert type(v) is defaultdict\n",
    "        #print(f\"DELETE FROM [{k}];\", file=f) # delete all rows\n",
    "        for kk,vv in v.items():\n",
    "            assert type(vv) is dict\n",
    "            s = '\\\",\\\"'.join(vvv for vvv in vv.keys()) # backslashes can't be included within format {}\n",
    "            print(f\"INSERT INTO \\\"{k}\\\" (\\\"{s}\\\")\", file=f)\n",
    "            values = []\n",
    "            for kkk,vvv in vv.items():\n",
    "                #print(kkk,vvv)\n",
    "                if type(vvv) is str:\n",
    "                    values.append('\\''+vvv.replace('\\'','\\'\\'')+'\\'') # escape embedded double-quotes with sql's double double-quotes\n",
    "                elif vvv is None:\n",
    "                    values.append(\"NULL\")\n",
    "                elif type(vvv) is list:\n",
    "                    #print(\"list:\",kkk)\n",
    "                    values.append('\\'['+','.join(str(vvvv).replace('\\'','\\'\\'') for vvvv in vvv)+']\\'') # escape embedded double-quotes with sql's double double-quotes\n",
    "                else:\n",
    "                    values.append(vvv)\n",
    "            print(f\"VALUES({','.join(str(vvvv) for vvvv in values)})\", file=f)\n",
    "            # DO UPDATE SET updated_at=excluded.updated_at\n",
    "            s = ','.join('\\\"'+vvv+'\\\"=excluded.\\\"'+vvv+'\\\"' for vvv in vv.keys()) # backslashes can't be included within format {}\n",
    "            print(f\"ON CONFLICT(id) DO UPDATE SET {s}\", file=f, end='')\n",
    "            #print('created_at' in vv.keys(),'updated_at' in vv.keys())\n",
    "            assert ('created_at' in vv.keys()) == ('updated_at' in vv.keys())\n",
    "            if 'created_at' in vv.keys():\n",
    "                print(f\"\\nWHERE excluded.\\\"updated_at\\\" > \\\"updated_at\\\" OR (excluded.\\\"updated_at\\\" = \\\"updated_at\\\" AND excluded.\\\"created_at\\\" > \\\"created_at\\\")\", file=f, end='')\n",
    "            print(\";\\n\",file=f)\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7358eda",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T08:56:12.686977Z",
     "start_time": "2021-07-09T08:50:08.350261Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# takes 7m to 140m to process 320,000 files.\n",
    "# takes 2m, or 7m to update 1 new month of 12,000 files or 11000s if starting from scratch (3h?).\n",
    "# todo:\n",
    "# how to get more more exception info in try/except?\n",
    "# move ACBL_Create_Tables.sql into options\n",
    "# move partial directory and rglob() into options\n",
    "# create cookbook of sql queries.\n",
    "# write sql script that creates tables and columns similar to Excel implementations.\n",
    "# initially delete all urls or just filtered urls. if filtered, then need to pass to execute_sql() below.\n",
    "# group urls by directory so number of files in directory can be displayed.\n",
    "# show overall file progress, dir to process, files within each directory to process:(1234/20000) dir:(3/266) file:(123/300)\n",
    "# if skip_existing_files and file exists, do a rglob for .sql files, create list of .json files not in .sql list.\n",
    "#     ... will be faster than a for loop.\n",
    "# Error: 'charmap' codec can't encode character '\\x9d' in position 84: character maps to <undefined>: type:PAIRS file:club-results/268011/details/211798.data.json\n",
    "\n",
    "initially_delete_all_output_files = False\n",
    "skip_existing_files = True\n",
    "starting_nfile = 0\n",
    "ending_nfile = 0\n",
    "event_types = [] # 'PAIRS', 'HOME_STYLE_PAIRS', 'INDIVIDUAL', 'TEAMS'\n",
    "#event_types = ['TEAMS']\n",
    "\n",
    "urls = []\n",
    "for path in acblPath.joinpath('club-results').rglob('*.data.json'): # fyi: PurePathPosix doesn't support glob/rglob\n",
    "    urls.append(path.relative_to('.'))\n",
    "    \n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/108571/details/280270.data.json']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/275966/details/99197.data.json']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/104034/details/100661.data.json','club-results/104034/details/100663.data.json']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/108571/details/191864.data.json']]\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/104034/details/100661.data.json']]\n",
    "\n",
    "total_execution_time = 0\n",
    "total_files_written = 0\n",
    "if ending_nfile == 0: ending_nfile = len(urls)\n",
    "filtered_urls = urls[starting_nfile:ending_nfile]\n",
    "total_urls = len(filtered_urls)\n",
    "start_time = time.time()\n",
    "\n",
    "# delete files first, using filtered list of urls\n",
    "if initially_delete_all_output_files:\n",
    "    for nfile,url in enumerate(filtered_urls):\n",
    "        sql_file = url.with_suffix('.sql')\n",
    "        sql_file.unlink(missing_ok=True)\n",
    "\n",
    "for nfile,url in enumerate(filtered_urls):\n",
    "    nfile += 1\n",
    "    #url = 'https://my.acbl.org/club-results/details/290003' # todo: insert code to extract json from script\n",
    "    #r = requests.get(url)\n",
    "    json_file = url\n",
    "    sql_file = url.with_suffix('.sql')\n",
    "    print(f\"Processing ({nfile}/{total_urls}): file:{json_file.as_posix()}\")\n",
    "    if skip_existing_files:\n",
    "        if sql_file.exists():\n",
    "           print(f\"Skipping: File exists:{sql_file.as_posix()}\")\n",
    "           continue\n",
    "    try:\n",
    "        data_json = None\n",
    "        with open(json_file, 'r') as f:\n",
    "            data_json = json.load(f)\n",
    "        #print(f\"Reading {json_file.as_posix()} dict len:{len(data_json)}\")\n",
    "        if len(event_types) > 0 and data_json['type'] not in event_types:\n",
    "            #print(f\"Skipping type:{data_json['type']}: file{json_file.as_posix()}\")\n",
    "            continue\n",
    "        tables = defaultdict(lambda :defaultdict(dict))\n",
    "        json_to_sql_walk(tables,\"events\",\"\",\"\",data_json) # \"events\" is the main table.\n",
    "        with open(sql_file,'w') as f:\n",
    "            CreateSqlFile(tables,f)\n",
    "        total_files_written += 1\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}: type:{data_json['type']} file:{url.as_posix()}\")\n",
    "    else:\n",
    "        print(f\"Writing: type:{data_json['type']} file:{sql_file.as_posix()}\")\n",
    "\n",
    "print(f\"All files processed:{total_urls} files written:{total_files_written} total time:{round(time.time()-start_time,2)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fad2a1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-07-09T18:18:59.432817Z",
     "start_time": "2021-07-09T08:56:12.686977Z"
    }
   },
   "outputs": [],
   "source": [
    "# notes:\n",
    "# takes 9h to process 300,000 files into 40GB sql file. Each sql file is processed in about .05s. Windows 11 2700x system.\n",
    "# 1) creating in memory db, executing scripts, then writing memory db to disk file is 100x faster than direct writes to file.\n",
    "# 2) Can be incredibly slow unless \"PRAGMA journal_mode=WAL;\" is used. Much, much faster with PRAGMA.\n",
    "# You can manually set, using sqlite3 command, journal_mode to WAL once and it will be persisted.\n",
    "# .open <file>\n",
    "# PRAGMA journal_mode=WAL;\n",
    "# .quit\n",
    "# Alternatively, connection can be opened in WAL mode. e.g. new SQLiteConnection(\"Data Source=\" + file + \";PRAGMA journal_mode=WAL;\")\n",
    "# None of these statements improve performance:\n",
    "#   \"PRAGMA locking_mode=exclusive;\"\n",
    "#   \"PRAGMA synchronous=NORMAL;\"\n",
    "#   db.commit()\n",
    "\n",
    "# todo:\n",
    "# Should .sql files use TRANSACTION and COMMIT to prevent partial updates?\n",
    "# what's wrong with this file? input termination: club-results/268011/details/211798.data.sql\n",
    "# collect errors, report at finish, sort by error type.\n",
    "# run VALIDATE or CHECK before/after file save?\n",
    "\n",
    "# options\n",
    "starting_nfile = 0 # beginning slice\n",
    "ending_nfile = 0 # ending slice. 0 means all files\n",
    "write_direct_to_disk = False # write directly to disk. Otherwise will write to memory and then backup to disk (100x faster).\n",
    "create_tables = True # drop and recreate all tables. Will wipe out all data.\n",
    "delete_db = True\n",
    "create_engine_echo = False\n",
    "perform_integrity_checks = False\n",
    "db_file = 'acbl-details.sqlite'\n",
    "db_memory_connection_string = 'sqlite://'\n",
    "create_tables_sql_file = \"acbl_create_tables.sql\"\n",
    "\n",
    "db_file_connection_string = 'sqlite:///'+acblPath.joinpath(db_file).as_posix()\n",
    "db_file_path = acblPath.joinpath(db_file)\n",
    "\n",
    "if write_direct_to_disk:\n",
    "    db_connection_string = db_file_connection_string # disk file based db\n",
    "else:\n",
    "    db_connection_string = db_memory_connection_string # memory based db\n",
    "\n",
    "if delete_db and sqlalchemy_utils.functions.database_exists(db_file_connection_string):\n",
    "    print(f\"Deleting db:{db_file_connection_string}\")\n",
    "    sqlalchemy_utils.functions.drop_database(db_file_connection_string) # warning: can't delete file if in use by another app (restart kernel).\n",
    "\n",
    "if not sqlalchemy_utils.functions.database_exists(db_connection_string):\n",
    "    print(f\"Creating db:{db_connection_string}\")\n",
    "    sqlalchemy_utils.functions.create_database(db_connection_string)\n",
    "    create_tables = True\n",
    "    \n",
    "engine = sqlalchemy.create_engine(db_connection_string, echo=create_engine_echo)\n",
    "raw_connection = engine.raw_connection()\n",
    "\n",
    "if create_tables:\n",
    "    print(f\"Creating tables from:{create_tables_sql_file}\")\n",
    "    with open(create_tables_sql_file, 'r') as f:\n",
    "        create_sql = f.read()\n",
    "    raw_connection.executescript(create_sql) # create tables\n",
    "\n",
    "urls = []\n",
    "for path in acblPath.joinpath('club-results').rglob('*.data.sql'): # fyi: PurePathPosix doesn't support glob/rglob\n",
    "    urls.append(path.relative_to('.'))\n",
    "\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/108571/details/280270.data.sql']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/275966/details/99197.data.sql']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/275966/details/98557.data.sql']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in ['club-results/104034/details/100661.data.sql','club-results/104034/details/100663.data.sql']] # use slashes, not backslashes\n",
    "#urls = [acblPath.joinpath(f) for f in 100*['club-results/108571/details/191864.data.sql']]\n",
    "\n",
    "total_script_execution_time = 0\n",
    "total_scripts_executed = 0\n",
    "canceled = False\n",
    "if ending_nfile == 0: ending_nfile = len(urls)\n",
    "filtered_urls = urls[starting_nfile:ending_nfile]\n",
    "total_filtered_urls = len(filtered_urls)\n",
    "start_time = time.time()\n",
    "for nfile,url in enumerate(filtered_urls):\n",
    "    nfile += 1\n",
    "    sql_file = url\n",
    "    print(f\"Executing SQL script ({nfile}/{total_filtered_urls}): file:{sql_file.as_posix()}\")\n",
    "    \n",
    "    try:\n",
    "        sql_script = None\n",
    "        with open(sql_file, 'r') as f:\n",
    "            sql_script = f.read()\n",
    "        start_script_time = time.time()\n",
    "        raw_connection.executescript(sql_script)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {type(e).__name__} while processing file:{url.as_posix()}\")\n",
    "        print(traceback.format_exc())\n",
    "        print(f\"Removing {url.as_posix()}\")\n",
    "        sql_file.unlink(missing_ok=True) # delete any bad files, fix issues, rerun.\n",
    "        continue # todo: log error.\n",
    "        #break\n",
    "    except KeyboardInterrupt as e:\n",
    "        print(f\"Error: {type(e).__name__} while processing file:{url.as_posix()}\")\n",
    "        print(traceback.format_exc())\n",
    "        canceled = True\n",
    "        break\n",
    "    else:\n",
    "        script_execution_time = time.time()-start_script_time\n",
    "        print(f\"SQL script executed: file:{url.as_posix()}: time:{round(script_execution_time,2)}\")\n",
    "        total_script_execution_time += script_execution_time\n",
    "        total_scripts_executed += 1\n",
    "\n",
    "print(f\"SQL scripts executed ({total_scripts_executed}/{total_filtered_urls}/{len(urls)}): total changes:{raw_connection.total_changes} total script execution time:{round(time.time()-start_time,2)}: avg script execution time:{round(total_script_execution_time/max(1,total_scripts_executed),2)}\")\n",
    "# if using memory db, write memory db to disk file.\n",
    "if not canceled:\n",
    "    if perform_integrity_checks:\n",
    "        # todo: research how to detect and display failures? Which checks are needed?\n",
    "        print(f\"Performing quick_check on file\")\n",
    "        raw_connection.execute(\"PRAGMA quick_check;\") # takes 7m on disk\n",
    "        print(f\"Performing foreign_key_check on file\")\n",
    "        raw_connection.execute(\"PRAGMA foreign_key_check;\") # takes 3m on disk\n",
    "        print(f\"Performing integrity_check on file\")\n",
    "        raw_connection.execute(\"PRAGMA integrity_check;\") # takes 25m on disk\n",
    "    if not write_direct_to_disk:\n",
    "        print(f\"Writing memory db to file:{db_file_connection_string}\")\n",
    "        engine_file = sqlalchemy.create_engine(db_file_connection_string)\n",
    "        raw_connection_file = engine_file.raw_connection()\n",
    "        raw_connection.backup(raw_connection_file.connection)\n",
    "        raw_connection_file.close()\n",
    "        engine_file.dispose()\n",
    "\n",
    "raw_connection.close()\n",
    "engine.dispose()\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a277fdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "output_auto_scroll": true,
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
